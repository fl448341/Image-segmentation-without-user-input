## Image segmentation without user input

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/18KZCx2z7JBpD64iSCuu84eKPeb78Pobw?usp=sharing)

This project implements an automated image segmentation pipeline that combines **Grad-CAM** interpretability with the **Segment Anything Model (SAM)**, eliminating the need for manual user prompts. A custom, pure-PyTorch implementation of Grad-CAM was developed to generate saliency maps, identifying discriminative regions for specific target classes within a synthetic dataset of geometric shapes. These saliency maps are processed to automatically extract coordinate prompts - specifically targeting global maximum activations - which are then fed into SAM to perform zero-shot segmentation. Two strategies were evaluated: using a single foreground point versus combining foreground and background points, both achieving a **Mean IoU exceeding 71%** against ground-truth masks. The results demonstrate that explainability tools can successfully automate the prompting of foundation models, with the entire pipeline executing end-to-end without human intervention.
